{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217f9d42",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "import shap\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f6d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler_np(arr):\n",
    "    mean_val = np.mean(arr)\n",
    "    std_val = np.std(arr)\n",
    "    scaled_arr = (arr - mean_val) / std_val\n",
    "    return scaled_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f519568",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A  = pd.read_csv(\"F:/New_Drugs/Li_Endo/Li_Endo_Training.csv\")  # Ganti dengan lokasi dataset A\n",
    "data_A1 = data_A.drop(columns=['Sample_ID', 'Drug_Name', 'risk_level', 'Vm_Peak', 'Ca_Peak', 'Vm_Resting', 'Ca_Diastole', 'qInward', 'CaD90', 'qNet', 'CaD50'])\n",
    "print(data_A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beda908",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_bfr = data_A1.drop(['risk_code'], axis=1)\n",
    "y_data_label = data_A1['risk_code']\n",
    "print(X_data_bfr)\n",
    "print(y_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_data = label_encoder.fit_transform(y_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = standard_scaler_np(X_data_bfr)\n",
    "# X_data=X_data_bfr\n",
    "print(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e0e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "class RBFLayer(Layer):\n",
    "    def __init__(self, units, gamma, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.gamma = K.cast_to_floatx(gamma)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.centers = self.add_weight(name='centers',\n",
    "                                       shape=(self.units, input_shape[1]),\n",
    "                                       initializer=RandomUniform(minval=-1, maxval=1),\n",
    "                                       trainable=True)\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        C = K.expand_dims(self.centers)\n",
    "        H = K.transpose(C - K.transpose(inputs))\n",
    "        return K.exp(-self.gamma * K.sum(H ** 2, axis=1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.units)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(RBFLayer, self).get_config()\n",
    "        config.update({'units': self.units, 'gamma': self.gamma})\n",
    "        return config\n",
    "\n",
    "# Define the RBF network creation function\n",
    "def create_rbf_model(input_shape, units_rbf, gamma):\n",
    "    model = Sequential()\n",
    "    model.add(RBFLayer(units_rbf, gamma, input_shape=(input_shape,)))\n",
    "    model.add(Dense(3, activation='softmax'))  # Assuming 3 classes\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "input_shape = X_data.shape[1]\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(units_rbf=10, gamma=1.0):\n",
    "    return create_rbf_model(input_shape=input_shape, units_rbf=units_rbf, gamma=gamma)\n",
    "\n",
    "# Wrap Keras model and use it in scikit-learn\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the grid search parameters\n",
    "param_grid = {\n",
    "    'units_rbf': [5, 10, 15],\n",
    "    'gamma': [0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "# Perform the grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)  # cv is 3 for grid search, but we'll do 5-fold later\n",
    "grid_result = grid.fit(X_data, y_data)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "best_params = grid_result.best_params_\n",
    "best_units_rbf = best_params['units_rbf']\n",
    "best_gamma = best_params['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Setup for stratified cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Assuming you have already defined X_data and y_data\n",
    "X_data_np = X_data.to_numpy() if isinstance(X_data, pd.DataFrame) else X_data  # Convert to numpy array if X_data is a DataFrame\n",
    "y_data_np = y_data if isinstance(y_data, np.ndarray) else y_data.to_numpy()  # Ensure y_data is a numpy array\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_dir = f\"F:/New_Drugs/Li_Endo/RBF6f_units_rbf-{best_units_rbf}_gamma-{best_gamma}_{current_time}/\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Continue with the StratifiedKFold setup\n",
    "fold_var = 1\n",
    "for train_index, test_index in skf.split(X_data_np, y_data_np):\n",
    "    \n",
    "    # Create the model with the best found parameters\n",
    "    model = create_rbf_model(X_data_np.shape[1], units_rbf=best_units_rbf, gamma=best_gamma)  # Example hyperparameters\n",
    "\n",
    "    # One-hot encode the outputs\n",
    "    y_train_encoded = to_categorical(y_data_np[train_index], num_classes=3)\n",
    "    y_test_encoded = to_categorical(y_data_np[test_index], num_classes=3)\n",
    "\n",
    "    # Define your checkpoint path including the fold_var\n",
    "    checkpoint_path = f\"{model_dir}/model_fold_{fold_var}_{{epoch:02d}}_{{val_accuracy:.2f}}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "    csv_logger = CSVLogger(f\"{model_dir}/training_log_fold_{fold_var}.csv\", append=True)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_data_np[train_index], y_train_encoded, validation_data=(X_data_np[test_index], y_test_encoded), epochs=1000, batch_size=32, callbacks=[checkpoint, csv_logger])\n",
    "\n",
    "    fold_var += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eeb1eb",
   "metadata": {},
   "source": [
    "# Explainable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "model_test = load_model(model_dir + \"model_fold_2_129_0.87.hdf5\", custom_objects={'RBFLayer': RBFLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0549d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['risk_level', 'risk_code', 'Sample_ID']\n",
    "encoded_df_column_list = X_data_test.columns.difference(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1109d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.KernelExplainer(model_test.predict, X_data.values[:1000])  # Using the first 100 examples as background\n",
    "shap_values = explainer.shap_values(X_data.values[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f8dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, plot_type=\"bar\", class_names= ['High-risk', 'Intermediate-risk', 'Low-risk'], feature_names = X_data_test.columns, show=False)\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Mean (|SHAP Value|)\")\n",
    "# plt.savefig(\"F:/7. Result/running_bu_yunen/ord/ANN_14_Feature2-node1_12-node2_11-oprimizer_adam/ANN_Sum_Feature_Importance.jpg\")\n",
    "# plt.savefig(\"my_dependence_plot.pdf\") # we can save a PDF of the figure if we want\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91961fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assuming 'shap_values' is a list of arrays, where each array corresponds to a class\n",
    "# and 'X_data_test_bfr.columns' gives you the feature names.\n",
    "\n",
    "# Colors and class names for each class\n",
    "class_colors = ['#EC2CAC', '#077CDB', '#7E8511']\n",
    "class_names = ['High-risk', 'Intermediate-risk', 'Low-risk']\n",
    "\n",
    "# Output directory for saving TIFF files\n",
    "output_directory = 'E:/backup_CML_1/New_Drugs/Li_Endo/Result_Revision_1/12f_knn_metric_euclidean_n_neighbors_3/'  # Change to your specific directory\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Prepare data for the stacked bar plot\n",
    "feature_names = X_data_test.columns\n",
    "num_features = len(feature_names)\n",
    "num_classes = len(shap_values)\n",
    "mean_abs_shap_values = np.abs(shap_values).mean(axis=1)  # Mean across samples for each class\n",
    "\n",
    "# Accumulate total SHAP values for each feature\n",
    "total_shap_values = np.sum(mean_abs_shap_values, axis=0)\n",
    "\n",
    "# Sort features by total SHAP values from highest to lowest\n",
    "sorted_indices = np.argsort(total_shap_values)  # The minus sign is for descending order\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "sorted_total_shap_values = total_shap_values[sorted_indices]\n",
    "\n",
    "# Create the sorted stacked bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars_data = np.zeros(num_features)  # Initialize the bottom of the bars\n",
    "\n",
    "for class_index in range(num_classes):\n",
    "    # Sort the mean values according to the sorted feature indices\n",
    "    sorted_mean_vals = mean_abs_shap_values[class_index][sorted_indices]\n",
    "    \n",
    "    # Plot sorted stacked bars\n",
    "    plt.barh(sorted_feature_names, sorted_mean_vals, left=bars_data, color=class_colors[class_index],\n",
    "             edgecolor='black', height=0.5, label=class_names[class_index])\n",
    "    bars_data += sorted_mean_vals  # Update the bottom for the next stack\n",
    "\n",
    "# Add text with the total SHAP value outside of each stacked bar\n",
    "for i, total_val in enumerate(sorted_total_shap_values):\n",
    "    plt.text(bars_data[i], i, f'{total_val:.2f}', ha='left', va='center', color='black', fontsize=8)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Mean (|SHAP Value|)')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Summary of SHAP Values by Class')\n",
    "plt.grid(axis='x')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Save the plot as a TIFF file with DPI 300\n",
    "output_file_path = os.path.join(model_dir, 'SHAP_Summary_Stacked_Plot2.tif')\n",
    "plt.savefig(output_file_path, format='tiff', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Plot saved successfully at: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(shap_values)):\n",
    "    shap.summary_plot(shap_values[i], X_data.values[:1000], feature_names = X_data_test_bfr.columns, show=False)\n",
    "    # plt.title(\"Distribution of SHAP values for each feature in the 'Intermediate-Risk' class\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.xlabel(\"SHAP Value\")\n",
    "    plt.savefig(\"F:/New_Drugs/Li_Endo/RBF_20240312-155004/SHAP_Value_HIL_Risk_{0}.jpg\".format(i))\n",
    "    # plt.savefig(\"my_dependence_plot.pdf\") # we can save a PDF of the figure if we want\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "for i in range(len(shap_values)):\n",
    "    # Menghitung expected value secara keseluruhan\n",
    "    overall_expected_value = explainer.expected_value[0]\n",
    "\n",
    "    # Menggabungkan SHAP values dari beberapa baris data\n",
    "    merged_shap_values = shap_values[i].mean(axis=0)\n",
    "\n",
    "    scaled_shap_values = merged_shap_values * 10000\n",
    "    \n",
    "    plt.figure(figsize=(10, 6)) \n",
    "\n",
    "    # Meringkas waterfall plot untuk semua baris data\n",
    "    shap.plots._waterfall.waterfall_legacy(overall_expected_value, scaled_shap_values, \n",
    "                                           feature_names = X_data_test_bfr.columns, max_display=20, show=False)\n",
    "#     plt.ylabel(\"Feature name\")\n",
    "#     plt.xlabel(\"Mean (SHAP Value)\")\n",
    "    plt.savefig(\"F:/New_Drugs/Li_Endo/RBF_20240312-155004/plot_HIL_Risk_{0}.tiff\".format(i), dpi=300, format='tiff')\n",
    "#     plt.savefig(\"E:/backup_CML_1/New_Drugs/Li_Endo/Result_Revision_1/12f_svm/plot_HIL_Risk_{0}.jpg\".format(i))\n",
    "    # plt.savefig(\"my_dependence_plot.pdf\") # we can save a PDF of the figure if we want\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7bc12",
   "metadata": {},
   "source": [
    "# 10,000 Times Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_testAs  = pd.read_csv(\"F:/New_Drugs/Li_Endo/Li_Endo_Testing.csv\")  # Ganti dengan lokasi dataset A\n",
    "data_testAs1 = data_testAs.drop(columns=['Sample_ID', 'Drug_Name', 'risk_level', 'risk_code'])\n",
    "data_testAs1 = standard_scaler_np(data_testAs1)\n",
    "data_name = data_testAs['Drug_Name']\n",
    "data_test_ID = data_testAs['Sample_ID'].astype(int)\n",
    "data_risk = data_testAs['risk_level']\n",
    "\n",
    "data_test_n = pd.concat([data_test_ID, data_testAs1, data_name, data_risk], axis=1)\n",
    "\n",
    "print(data_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2fa8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Load model ANN Anda yang sudah dilatih\n",
    "model = model_test\n",
    "\n",
    "# Daftar unik dari jenis 'Drug_Name' dalam dataset\n",
    "unique_drug_names = data_test_n['Drug_Name'].unique()\n",
    "\n",
    "# Inisialisasi list untuk menyimpan semua confusion matrix\n",
    "all_confusion_matrices = []\n",
    "drug_combination = []\n",
    "d_X_test = []\n",
    "accu = []\n",
    "accu_h = []\n",
    "accu_i = []\n",
    "accu_l = []\n",
    "f1_sc = []\n",
    "f1_sc_h = []\n",
    "f1_sc_i = []\n",
    "f1_sc_l = []\n",
    "lr_p_h = []\n",
    "lr_p_i = []\n",
    "lr_p_l = []\n",
    "lr_n_h = []\n",
    "lr_n_i = []\n",
    "lr_n_l = []\n",
    "auc_high = []\n",
    "auc_inter = []\n",
    "auc_low = []\n",
    "\n",
    "# Lakukan iterasi sebanyak 10.000 kali\n",
    "for _ in range(10000):\n",
    "    # Pilih secara acak 16 'Drug_Name' yang berbeda\n",
    "    unique_drug_names = np.random.choice(data_test_n['Drug_Name'].unique(), 16, replace=False)\n",
    "\n",
    "    # Inisialisasi list untuk menyimpan 16 sampel\n",
    "    selected_combinations = []\n",
    "\n",
    "    # Memilih satu sampel dengan 'Drug_Name' yang sesuai untuk setiap 'Drug_Name' yang telah dipilih\n",
    "    for drug_name in unique_drug_names:\n",
    "        selected_sample = data_test_n[data_test_n['Drug_Name'] == drug_name].sample(1)\n",
    "        selected_combinations.append(selected_sample)\n",
    "\n",
    "    # Menggabungkan 16 sampel menjadi satu DataFrame\n",
    "    selected_combinations = pd.concat(selected_combinations)\n",
    "\n",
    "    # Pisahkan fitur dan target sesuai kebutuhan Anda\n",
    "    X_test = selected_combinations.drop(columns=['Sample_ID', 'Drug_Name', 'risk_level', 'Vm_Peak', 'Ca_Peak', 'Vm_Resting', 'Ca_Diastole', 'qInward', 'CaD90', 'qNet', 'CaD50'])  # Sesuaikan dengan struktur dataset Anda\n",
    "    y_test = selected_combinations['risk_level']  # Sesuaikan dengan struktur dataset Anda\n",
    "    y_test = label_encoder.fit_transform(y_test)\n",
    "    \n",
    "    # Uji model dengan data uji\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Hitung confusion matrix\n",
    "    cm = confusion_matrix(y_test, np.argmax(y_pred, axis=1))  # Sesuaikan dengan nilai threshold yang sesuai\n",
    "#     print(cm)\n",
    "    \n",
    "    tp_high = cm[0,0]\n",
    "    tn_high = cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2]\n",
    "    fp_high = cm[1,0]+cm[2,0]\n",
    "    fn_high = cm[0,1]+cm[0,2]\n",
    "\n",
    "    tp_inter = cm[1,1]\n",
    "    tn_inter = cm[0,0]+cm[0,2]+cm[2,0]+cm[2,2]\n",
    "    fp_inter = cm[0,1]+cm[2,1]\n",
    "    fn_inter = cm[1,0]+cm[1,2]\n",
    "\n",
    "    tp_low = cm[2,2]\n",
    "    tn_low = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "    fp_low = cm[0,2]+cm[1,2]\n",
    "    fn_low = cm[2,0]+cm[2,1]\n",
    "\n",
    "    acc_high = (tp_high+tn_high)/(tp_high+tn_high+fp_high+fn_high)\n",
    "    pre_high = tp_high/(tp_high+fp_high)\n",
    "    rec_high = tp_high/(tp_high+fn_high)\n",
    "    spe_high = tn_high/(tn_high+fp_high)\n",
    "    lrp_high = rec_high/(1-spe_high)\n",
    "    lrn_high = (1-rec_high)/spe_high\n",
    "    f1s_high = (2*pre_high*rec_high)/(pre_high+rec_high)\n",
    "\n",
    "    acc_inter = (tp_inter+tn_inter)/(tp_inter+tn_inter+fp_inter+fn_inter)\n",
    "    pre_inter = tp_inter/(tp_inter+fp_inter)\n",
    "    rec_inter = tp_inter/(tp_inter+fn_inter)\n",
    "    spe_inter = tn_inter/(tn_inter+fp_inter)\n",
    "    lrp_inter = rec_inter/(1-spe_inter)\n",
    "    lrn_inter = (1-rec_inter)/spe_inter\n",
    "    f1s_inter = (2*pre_inter*rec_inter)/(pre_inter+rec_inter)\n",
    "\n",
    "    acc_low = (tp_low+tn_low)/(tp_low+tn_low+fp_low+fn_low)\n",
    "    pre_low = tp_low/(tp_low+fp_low)\n",
    "    rec_low = tp_low/(tp_low+fn_low)\n",
    "    spe_low = tn_low/(tn_low+fp_low)\n",
    "    lrp_low = rec_low/(1-spe_low)\n",
    "    lrn_low = (1-rec_low)/spe_low\n",
    "    f1s_low = (2*pre_low*rec_low)/(pre_low+rec_low)\n",
    "    \n",
    "    acc = (acc_high+acc_inter+acc_low)/3\n",
    "    f1_s = (f1s_high+f1s_inter+f1s_low)/3\n",
    "    \n",
    "#     fpr_high, tpr_high, _ = roc_curve(y_test == 0, y_pred[:, 0])\n",
    "#     roc_auc_high = auc(fpr_high, tpr_high)\n",
    "    \n",
    "\n",
    "#     fpr_inter, tpr_inter, _ = roc_curve(y_test == 1, y_pred[:, 1])\n",
    "#     roc_auc_inter = auc(fpr_inter, tpr_inter)\n",
    "    \n",
    "\n",
    "#     fpr_low, tpr_low, _ = roc_curve(y_test == 2, y_pred[:, 2])\n",
    "#     roc_auc_low = auc(fpr_low, tpr_low)\n",
    "    \n",
    "    roc_auc_high = roc_auc_score(y_test == 0, y_pred[:, 0])\n",
    "    roc_auc_inter = roc_auc_score(y_test == 1, y_pred[:, 1])\n",
    "    roc_auc_low = roc_auc_score(y_test == 2, y_pred[:, 2])\n",
    "    \n",
    "#     if acc >= 0.70:\n",
    "    auc_high.append(roc_auc_high)\n",
    "    auc_inter.append(roc_auc_inter)\n",
    "    auc_low.append(roc_auc_low)\n",
    "\n",
    "    # Simpan confusion matrix ke dalam list\n",
    "    drug_combination.append(selected_combinations)\n",
    "    all_confusion_matrices.append(cm)\n",
    "    d_X_test.append(X_test)\n",
    "    accu.append(acc)\n",
    "    accu_h.append(acc_high)\n",
    "    accu_i.append(acc_inter)\n",
    "    accu_l.append(acc_low)\n",
    "    f1_sc.append(f1_s)\n",
    "    f1_sc_h.append(f1s_high)\n",
    "    f1_sc_i.append(f1s_inter)\n",
    "    f1_sc_l.append(f1s_low)\n",
    "    lr_p_h.append(lrp_high)\n",
    "    lr_p_i.append(lrp_inter)\n",
    "    lr_p_l.append(lrp_low)\n",
    "    lr_n_h.append(lrn_high)\n",
    "    lr_n_i.append(lrn_inter)\n",
    "    lr_n_l.append(lrn_low)\n",
    "    \n",
    "# Buat DataFrame dari list\n",
    "df_result = pd.DataFrame({'Drug_Combination': drug_combination, \n",
    "                        'X_data': d_X_test, \n",
    "                        'Confusion_Matrix': all_confusion_matrices,\n",
    "                        'Acc' : accu,\n",
    "                        'Acc_High': accu_h,\n",
    "                         'Acc_Inter': accu_i,\n",
    "                         'Acc_Low': accu_l,\n",
    "                        'AUC_High': auc_high,\n",
    "                        'AUC_Inter': auc_inter,\n",
    "                        'AUC_Low': auc_low,\n",
    "                         'F1_S': f1_sc,\n",
    "                         'F1_S_High': f1_sc_h,\n",
    "                         'F1_S_Inter': f1_sc_i,\n",
    "                         'F1_S_Low': f1_sc_l,\n",
    "                         'LR_p_High': lr_p_h,\n",
    "                         'LR_p_Inter': lr_p_i,\n",
    "                         'LR_p_Low': lr_p_l,\n",
    "                         'LR_n_High': lr_n_h,\n",
    "                         'LR_n_Inter': lr_n_i,\n",
    "                         'LR_n_Low': lr_n_l})\n",
    "\n",
    "# df_result_filtered = df_result[df_result['Acc'] >= 0.70]\n",
    "\n",
    "df_result.to_csv(model_dir + 'Result_of_10000_Times_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef906956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Define custom colors and edge colors\n",
    "color_high = '#EC2CAC'  # Custom color for AUC_High\n",
    "color_inter = '#077CDB'  # Custom color for AUC_Inter\n",
    "color_low = '#7E8511'  # Custom color for AUC_Low\n",
    "edge_color = 'black'  # Edge color\n",
    "\n",
    "# Create a figure for the histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram for 'AUC_High'\n",
    "plt.subplot(131)\n",
    "plt.hist(auc_high, bins=20, color=color_high, edgecolor=edge_color)\n",
    "plt.title('AUC_High Histogram')\n",
    "plt.xlabel('AUC_High')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for 'AUC_Inter'\n",
    "plt.subplot(132)\n",
    "plt.hist(auc_inter, bins=20, color=color_inter, edgecolor=edge_color)\n",
    "plt.title('AUC_Inter Histogram')\n",
    "plt.xlabel('AUC_Inter')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for 'AUC_Low'\n",
    "plt.subplot(133)\n",
    "plt.hist(auc_low, bins=20, color=color_low, edgecolor=edge_color)\n",
    "plt.title('AUC_Low Histogram')\n",
    "plt.xlabel('AUC_Low')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()  # Ensure the subplots don't overlap\n",
    "\n",
    "# Define the file path where you want to save the histograms\n",
    "output_file_path = model_dir + 'AUC.png'\n",
    "\n",
    "# Save the figure to the specified file\n",
    "plt.savefig(output_file_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plots (optional)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_result is your DataFrame\n",
    "columns_of_interest = ['Acc', 'Acc_High', 'Acc_Inter', 'Acc_Low', 'AUC_High', 'AUC_Inter', 'AUC_Low', 'F1_S', 'F1_S_High', 'F1_S_Inter', 'F1_S_Low', 'LR_p_High', 'LR_p_Inter', 'LR_p_Low', 'LR_n_High', 'LR_n_Inter', 'LR_n_Low']\n",
    "\n",
    "# Calculate the median and confidence interval\n",
    "medians = df_result[columns_of_interest].median()\n",
    "lower_bound = df_result[columns_of_interest].quantile(0.025)\n",
    "upper_bound = df_result[columns_of_interest].quantile(0.975)\n",
    "\n",
    "# Combine median, lower, and upper into a single string with two decimal places\n",
    "result_strings = medians.round(2).astype(str) + ' (' + lower_bound.round(2).astype(str) + ', ' + upper_bound.round(2).astype(str) + ')'\n",
    "\n",
    "# Create a new DataFrame with the desired format\n",
    "table = pd.DataFrame({'Median (Lower_CI, Upper_CI)': result_strings})\n",
    "\n",
    "# Add a new column for the metric names\n",
    "table['auc_detail'] = columns_of_interest\n",
    "\n",
    "# Reorder the columns\n",
    "table = table[['auc_detail', 'Median (Lower_CI, Upper_CI)']]\n",
    "# Save the table to Excel\n",
    "table.to_excel(model_dir + 'Final_result_RF_01.xlsx', index=False)\n",
    "\n",
    "# Display the table\n",
    "table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
