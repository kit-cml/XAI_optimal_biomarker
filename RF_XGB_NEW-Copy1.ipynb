{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4308044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "import shap\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195174c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dVm/dt_repol  Vm_Resting       APD90       APD50  APDtri       CaD90  \\\n",
      "0         -0.300345  -88.002062  391.600435  318.450435  73.150  688.500001   \n",
      "1         -0.317049  -88.003051  370.700439  303.150439  67.550  682.500001   \n",
      "2         -0.279408  -87.999663  420.275299  340.875299  79.400  686.000001   \n",
      "3         -0.255510  -88.001840  455.900506  365.575506  90.325  709.500001   \n",
      "4         -0.298442  -88.001574  394.375319  321.325319  73.050  684.500001   \n",
      "...             ...         ...         ...         ...     ...         ...   \n",
      "21995     -0.298215  -88.039405  350.900470  281.325470  69.575  817.500001   \n",
      "21996     -0.283611  -88.036115  371.725455  297.850455  73.875  815.500001   \n",
      "21997     -0.290358  -88.034500  366.000421  293.750421  72.250  806.000001   \n",
      "21998     -0.281967  -88.034736  374.300405  300.075404  74.225  810.500001   \n",
      "21999     -0.280863  -88.034116  376.475531  301.800531  74.675  809.000001   \n",
      "\n",
      "           qNet   qInward  risk_code  \n",
      "0      0.069732  0.992629          0  \n",
      "1      0.074504  0.978760          0  \n",
      "2      0.061599  1.008048          0  \n",
      "3      0.056438  1.005769          0  \n",
      "4      0.068459  0.994693          0  \n",
      "...         ...       ...        ...  \n",
      "21995  0.086955  0.876913          2  \n",
      "21996  0.082218  0.894993          2  \n",
      "21997  0.083018  0.896238          2  \n",
      "21998  0.081290  0.899692          2  \n",
      "21999  0.081085  0.889231          2  \n",
      "\n",
      "[22000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "data_A  = pd.read_csv(\"F:/New_Drugs/Li_Endo/Li_Endo_Training.csv\")  # Ganti dengan lokasi dataset A\n",
    "data_A1 = data_A.drop(columns=['Sample_ID', 'Drug_Name', 'risk_level', 'Vm_Peak', 'Ca_Peak','Ca_Diastole','Max_dVm/dt','Catri','CaD50'])\n",
    "print(data_A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c1b9fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dVm/dt_repol  Vm_Resting       APD90       APD50  APDtri       CaD90  \\\n",
      "0         -0.300345  -88.002062  391.600435  318.450435  73.150  688.500001   \n",
      "1         -0.317049  -88.003051  370.700439  303.150439  67.550  682.500001   \n",
      "2         -0.279408  -87.999663  420.275299  340.875299  79.400  686.000001   \n",
      "3         -0.255510  -88.001840  455.900506  365.575506  90.325  709.500001   \n",
      "4         -0.298442  -88.001574  394.375319  321.325319  73.050  684.500001   \n",
      "...             ...         ...         ...         ...     ...         ...   \n",
      "21995     -0.298215  -88.039405  350.900470  281.325470  69.575  817.500001   \n",
      "21996     -0.283611  -88.036115  371.725455  297.850455  73.875  815.500001   \n",
      "21997     -0.290358  -88.034500  366.000421  293.750421  72.250  806.000001   \n",
      "21998     -0.281967  -88.034736  374.300405  300.075404  74.225  810.500001   \n",
      "21999     -0.280863  -88.034116  376.475531  301.800531  74.675  809.000001   \n",
      "\n",
      "           qNet   qInward  \n",
      "0      0.069732  0.992629  \n",
      "1      0.074504  0.978760  \n",
      "2      0.061599  1.008048  \n",
      "3      0.056438  1.005769  \n",
      "4      0.068459  0.994693  \n",
      "...         ...       ...  \n",
      "21995  0.086955  0.876913  \n",
      "21996  0.082218  0.894993  \n",
      "21997  0.083018  0.896238  \n",
      "21998  0.081290  0.899692  \n",
      "21999  0.081085  0.889231  \n",
      "\n",
      "[22000 rows x 8 columns]\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "21995    2\n",
      "21996    2\n",
      "21997    2\n",
      "21998    2\n",
      "21999    2\n",
      "Name: risk_code, Length: 22000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_data_bfr = data_A1.drop(['risk_code'], axis=1)\n",
    "y_data_label = data_A1['risk_code']\n",
    "print(X_data_bfr)\n",
    "print(y_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d6a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [5],\n",
    "    'min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=skf, n_jobs=-1)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid.fit(X_data_bfr, y_data_label)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid.best_params_\n",
    "\n",
    "# Create a folder name based on the best parameters\n",
    "folder_name = f\"E:/backup_CML_1/New_Drugs/Li_Endo/Result_Revision_2/RF_6_feature_nestimators_{best_params['n_estimators']}_maxdepth_{best_params['max_depth']}_minsplit_{best_params['min_samples_split']}_minleaf_{best_params['min_samples_leaf']}\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Train the best model using 10-fold cross-validation and save models\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X_data_bfr, y_data_label):\n",
    "    fold += 1\n",
    "    X_train, X_test = X_data_bfr.iloc[train_index], X_data_bfr.iloc[test_index]\n",
    "    y_train, y_test = y_data_label[train_index], y_data_label[test_index]\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42, **best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Fold {fold} - Akurasi: {accuracy}\")\n",
    "\n",
    "    # Save the model to the folder\n",
    "    model_filename = os.path.join(folder_name, f\"model_fold_{fold}.joblib\")\n",
    "    joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46163824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Akurasi: 0.9840909090909091\n",
      "Fold 2 - Akurasi: 0.9845454545454545\n",
      "Fold 3 - Akurasi: 0.9852272727272727\n",
      "Fold 4 - Akurasi: 0.9843181818181819\n",
      "Fold 5 - Akurasi: 0.9872727272727273\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [250],\n",
    "    'max_depth': [4],  # XGBoost uses max_depth instead of max_depth\n",
    "    'min_child_weight': [3],  # Similar to min_samples_split in RandomForest\n",
    "    'gamma': [0.3],  # Regularization parameter\n",
    "    'subsample': [0.3],  # Fraction of samples used for fitting trees\n",
    "    'colsample_bytree': [1],  # Fraction of features used for fitting trees\n",
    "    'learning_rate': [0.3]  # Step size shrinkage used to prevent overfitting\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid = GridSearchCV(estimator=xgb.XGBClassifier(random_state=42), param_grid=param_grid, cv=skf, n_jobs=-1)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid.fit(X_data_bfr, y_data_label)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid.best_params_\n",
    "\n",
    "# Create a folder name based on the best parameters\n",
    "folder_name = f\"E:/backup_CML_1/New_Drugs/Li_Endo/Result_Revision_2/XGB_8_feature_nestimators_{best_params['n_estimators']}_maxdepth_{best_params['max_depth']}_minchild_{best_params['min_child_weight']}_gamma_{best_params['gamma']}_subsample_{best_params['subsample']}_colsample_{best_params['colsample_bytree']}_lr_{best_params['learning_rate']}\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Train the best model using 10-fold cross-validation and save models\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X_data_bfr, y_data_label):\n",
    "    fold += 1\n",
    "    X_train, X_test = X_data_bfr.iloc[train_index], X_data_bfr.iloc[test_index]\n",
    "    y_train, y_test = y_data_label[train_index], y_data_label[test_index]\n",
    "\n",
    "    model = xgb.XGBClassifier(random_state=42, **best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Fold {fold} - Akurasi: {accuracy}\")\n",
    "\n",
    "    # Save the model to the folder\n",
    "    model_filename = os.path.join(folder_name, f\"model_fold_{fold}.joblib\")\n",
    "    joblib.dump(model, model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430be66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_testA  = pd.read_csv(\"F:/7. Result/running_bu_yunen/duta_testing_ORD.csv\")  # Ganti dengan lokasi dataset A\n",
    "data_testA1 = data_testA.drop(columns=['Sample_ID', 'Drug', 'risk_level'])\n",
    "print(data_testA1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c09554",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_test_bfr = data_testA1.drop(['risk_code'], axis=1)\n",
    "y_data_test_lbl = data_testA1['risk_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea94ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the Random Forest model\n",
    "model_dir = \"F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/\"\n",
    "\n",
    "# List all model files in the directory\n",
    "model_files = glob.glob(os.path.join(model_dir, '*.joblib'))\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each model\n",
    "for model_file in model_files:\n",
    "    # Load the Random Forest model\n",
    "    model = joblib.load(model_file)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_data_test_bfr)\n",
    "\n",
    "    # Calculate evaluation metrics (e.g., accuracy and F1 score)\n",
    "    accuracy = accuracy_score(y_data_test_lbl, y_pred)\n",
    "    f1 = f1_score(y_true=y_data_test_lbl, y_pred=y_pred, average='weighted')  # Or 'micro', 'macro', etc., as needed\n",
    "\n",
    "    # Save the test results\n",
    "    result = {\n",
    "        'Model': os.path.basename(model_file),\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Create a dataframe from the test results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/Hasil_one_test_ORd_01.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9346558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "model_test = joblib.load('F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/model_fold_2.joblib')\n",
    "# model_test = load_model(\"F:/7. Result/Chantest_model/save_model_02-node1_12-node2_8-oprimizer_rmsprop/Model-20231017-135318-fold-4-epoch-3.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21bb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_test)\n",
    "shap_values = explainer.shap_values(X_data_test_bfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, plot_type=\"bar\", feature_names = X_train.columns, show=False)\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Mean (|SHAP Value|)\")\n",
    "plt.savefig(\"F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/XGB_Sum_Feature_Importance.jpg\")\n",
    "# plt.savefig(\"my_dependence_plot.pdf\") # we can save a PDF of the figure if we want\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc329c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, plot_type=\"bar\", class_names= ['High-risk', 'Intermediate-risk', 'Low-risk'], feature_names = X_train.columns, show=False)\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Mean (|SHAP Value|)\")\n",
    "plt.savefig(\"F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/XGB_Sum_Feature_Importance_label.jpg\")\n",
    "# plt.savefig(\"my_dependence_plot.pdf\") # we can save a PDF of the figure if we want\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Daftar warna yang akan digunakan, satu warna untuk setiap kelas\n",
    "class_colors = ['#077CDB', '#EC2CAC', '#7E8511']\n",
    "\n",
    "# Output directory for saving TIFF files\n",
    "output_directory = 'F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterasi untuk setiap kelas\n",
    "for class_index in range(len(shap_values)):\n",
    "    j = str(class_index)\n",
    "    \n",
    "    # Menghitung expected value secara keseluruhan\n",
    "    overall_expected_value = explainer.expected_value[0]\n",
    "\n",
    "    # Menggabungkan SHAP values dari beberapa baris data (tanpa nilai absolut)\n",
    "    merged_shap_values = np.mean(np.abs(shap_values[class_index]), axis=0)\n",
    "\n",
    "    scaled_shap_values = merged_shap_values\n",
    "\n",
    "    # Memilih warna berdasarkan indeks kelas\n",
    "    class_color = class_colors[class_index % len(class_colors)]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(X_data_test_bfr.columns, scaled_shap_values, color=class_color)\n",
    "    plt.xlabel('SHAP Values')\n",
    "    plt.ylabel('Feature Names')\n",
    "    plt.title(\"SHAP Values for Class_\" + j)\n",
    "    plt.grid(axis='x')\n",
    "    # Menambahkan nilai detail pada setiap batang\n",
    "    # Menambahkan nilai detail pada setiap batang di luar batang, di paling kanan\n",
    "    for bar, shap_value in zip(bars, scaled_shap_values):\n",
    "        plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{shap_value:.3f}', ha='left', va='center', color='black')\n",
    "\n",
    "    # Save the plot as a TIFF file with DPI 300\n",
    "    output_file_path = os.path.join(output_directory, f'SHAP_Plot_Class_{j}.tif')\n",
    "    plt.savefig(output_file_path, format='tiff', dpi=300)\n",
    "    plt.close()  # Close the plot to avoid displaying it in the notebook\n",
    "\n",
    "print(\"Plots saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4130f46b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Daftar warna yang akan digunakan, satu warna untuk setiap kelas\n",
    "class_colors = ['#7E8511','#077CDB', '#EC2CAC']\n",
    "\n",
    "# Iterasi untuk setiap kelas\n",
    "for class_index in range(len(shap_values)):\n",
    "    j = str(class_index)\n",
    "    \n",
    "    # Menghitung expected value secara keseluruhan\n",
    "    overall_expected_value = explainer.expected_value[0]\n",
    "\n",
    "    # Menggabungkan SHAP values dari beberapa baris data (tanpa nilai absolut)\n",
    "    merged_shap_values = np.mean(np.abs(shap_values[class_index]), axis=0)\n",
    "\n",
    "    scaled_shap_values = merged_shap_values\n",
    "\n",
    "    # Memilih warna berdasarkan indeks kelas\n",
    "    class_color = class_colors[class_index % len(class_colors)]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(X_data_test_bfr.columns, scaled_shap_values, color=class_color)\n",
    "    plt.xlabel('SHAP Values')\n",
    "    plt.ylabel('Feature Names')\n",
    "    plt.title(\"SHAP Values for Class_\" + j)\n",
    "    plt.grid(axis='x')\n",
    "    # Menambahkan nilai detail pada setiap batang\n",
    "    # Menambahkan nilai detail pada setiap batang di luar batang, di paling kanan\n",
    "    for bar, shap_value in zip(bars, scaled_shap_values):\n",
    "        plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{shap_value:.3f}', ha='left', va='center', color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "for i in range(len(shap_values)):\n",
    "    # Menghitung expected value secara keseluruhan\n",
    "    overall_expected_value = explainer.expected_value[0]\n",
    "\n",
    "    # Menggabungkan SHAP values dari beberapa baris data\n",
    "    merged_shap_values = shap_values[i].mean(axis=0)\n",
    "\n",
    "    scaled_shap_values = merged_shap_values * 10\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Meringkas waterfall plot untuk semua baris data\n",
    "    shap.plots._waterfall.waterfall_legacy(overall_expected_value, scaled_shap_values, \n",
    "                                           feature_names = X_data_test_bfr.columns, max_display=20, show=False)\n",
    "    plt.savefig(output_directory + \"Waterfall_HIL_Risk_{0}.jpg\".format(i))\n",
    "    # plt.savefig(\"my_dependence_plot.pdf\") # we can save a PDF of the figure if we want\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e7112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_testAs  = pd.read_csv(\"F:/7. Result/running_bu_yunen/duta_testing_ORD.csv\")  # Ganti dengan lokasi dataset A\n",
    "data_testAs1 = data_testAs.drop(columns=['Sample_ID', 'Drug', 'risk_level'])\n",
    "data_name = data_testAs['Drug']\n",
    "data_test_ID = data_testAs['Sample_ID'].astype(int)\n",
    "\n",
    "data_test_n = pd.concat([data_test_ID, data_testAs1, data_name], axis=1)\n",
    "\n",
    "print(data_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89225bab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve, auc  # Import roc_curve and auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib  # Import modul joblib\n",
    "\n",
    "# Load the Random Forest model using joblib\n",
    "model = joblib.load('F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/model_fold_2.joblib')  # Ganti dengan path yang sesuai\n",
    "\n",
    "# Daftar unik dari jenis 'Drug_Name' dalam dataset\n",
    "unique_drug_names = data_test_n['Drug'].unique()\n",
    "\n",
    "# Inisialisasi list untuk menyimpan semua confusion matrix\n",
    "all_confusion_matrices = []\n",
    "drug_combination = []\n",
    "d_X_test = []\n",
    "accu = []\n",
    "accu_h = []\n",
    "accu_i = []\n",
    "accu_l = []\n",
    "f1_sc = []\n",
    "f1_sc_h = []\n",
    "f1_sc_i = []\n",
    "f1_sc_l = []\n",
    "lr_p_h = []\n",
    "lr_p_i = []\n",
    "lr_p_l = []\n",
    "lr_n_h = []\n",
    "lr_n_i = []\n",
    "lr_n_l = []\n",
    "auc_high = []\n",
    "auc_inter = []\n",
    "auc_low = []\n",
    "rec_h = []\n",
    "rec_i = []\n",
    "rec_l = []\n",
    "spe_h = []\n",
    "spe_i = []\n",
    "spe_l = []\n",
    "\n",
    "# Lakukan iterasi sebanyak 10.000 kali\n",
    "for _ in range(10000):\n",
    "    # Pilih secara acak 16 'Drug_Name' yang berbeda\n",
    "    unique_drug_names = np.random.choice(data_test_n['Drug'].unique(), 16, replace=False)\n",
    "\n",
    "    # Inisialisasi list untuk menyimpan 16 sampel\n",
    "    selected_combinations = []\n",
    "\n",
    "    # Memilih satu sampel dengan 'Drug_Name' yang sesuai untuk setiap 'Drug_Name' yang telah dipilih\n",
    "    for drug_name in unique_drug_names:\n",
    "        selected_sample = data_test_n[data_test_n['Drug'] == drug_name].sample(1)\n",
    "        selected_combinations.append(selected_sample)\n",
    "\n",
    "    # Menggabungkan 16 sampel menjadi satu DataFrame\n",
    "    selected_combinations = pd.concat(selected_combinations)\n",
    "\n",
    "    # Pisahkan fitur dan target sesuai kebutuhan Anda\n",
    "    X_test = selected_combinations.drop(columns=['Sample_ID', 'risk_code', 'Drug'])  # Sesuaikan dengan struktur dataset Anda\n",
    "    y_test = selected_combinations['risk_code']  # Sesuaikan dengan struktur dataset Anda\n",
    "#     y_test = label_encoder.fit_transform(y_test)\n",
    "    \n",
    "    # Uji model dengan data uji\n",
    "    y_pred = model.predict_proba(X_test)   # Menggunakan model Random Forest yang telah dimuat\n",
    "\n",
    "    # Uji model dengan data uji\n",
    "    y_pred1 = model.predict(X_test)\n",
    "    \n",
    "    # Hitung confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred1)  # Sesuaikan dengan nilai threshold yang sesuai\n",
    "    \n",
    "    tp_high = cm[0,0]\n",
    "    tn_high = cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2]\n",
    "    fp_high = cm[1,0]+cm[2,0]\n",
    "    fn_high = cm[0,1]+cm[0,2]\n",
    "\n",
    "    tp_inter = cm[1,1]\n",
    "    tn_inter = cm[0,0]+cm[0,2]+cm[2,0]+cm[2,2]\n",
    "    fp_inter = cm[0,1]+cm[2,1]\n",
    "    fn_inter = cm[1,0]+cm[1,2]\n",
    "\n",
    "    tp_low = cm[2,2]\n",
    "    tn_low = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "    fp_low = cm[0,2]+cm[1,2]\n",
    "    fn_low = cm[2,0]+cm[2,1]\n",
    "\n",
    "    acc_high = (tp_high+tn_high)/(tp_high+tn_high+fp_high+fn_high)\n",
    "    pre_high = tp_high/(tp_high+fp_high)\n",
    "    rec_high = tp_high/(tp_high+fn_high)\n",
    "    spe_high = tn_high/(tn_high+fp_high)\n",
    "    lrp_high = rec_high/(1-spe_high)\n",
    "    lrn_high = (1-rec_high)/spe_high\n",
    "    f1s_high = (2*pre_high*rec_high)/(pre_high+rec_high)\n",
    "\n",
    "    acc_inter = (tp_inter+tn_inter)/(tp_inter+tn_inter+fp_inter+fn_inter)\n",
    "    pre_inter = tp_inter/(tp_inter+fp_inter)\n",
    "    rec_inter = tp_inter/(tp_inter+fn_inter)\n",
    "    spe_inter = tn_inter/(tn_inter+fp_inter)\n",
    "    lrp_inter = rec_inter/(1-spe_inter)\n",
    "    lrn_inter = (1-rec_inter)/spe_inter\n",
    "    f1s_inter = (2*pre_inter*rec_inter)/(pre_inter+rec_inter)\n",
    "\n",
    "    acc_low = (tp_low+tn_low)/(tp_low+tn_low+fp_low+fn_low)\n",
    "    pre_low = tp_low/(tp_low+fp_low)\n",
    "    rec_low = tp_low/(tp_low+fn_low)\n",
    "    spe_low = tn_low/(tn_low+fp_low)\n",
    "    lrp_low = rec_low/(1-spe_low)\n",
    "    lrn_low = (1-rec_low)/spe_low\n",
    "    f1s_low = (2*pre_low*rec_low)/(pre_low+rec_low)\n",
    "    \n",
    "    acc = (acc_high+acc_inter+acc_low)/3\n",
    "    f1_s = (f1s_high+f1s_inter+f1s_low)/3\n",
    "    \n",
    "# Hitung AUC untuk masing-masing kelas\n",
    "    auc_high_value = roc_auc_score(y_test == 0, y_pred[:, 0])\n",
    "    auc_inter_value = roc_auc_score(y_test == 1, y_pred[:, 1])\n",
    "    auc_low_value = roc_auc_score(y_test == 2, y_pred[:, 2])\n",
    "\n",
    "    auc_high.append(auc_high_value)\n",
    "    auc_inter.append(auc_inter_value)\n",
    "    auc_low.append(auc_low_value)\n",
    "\n",
    "\n",
    "    # Simpan confusion matrix ke dalam list\n",
    "    drug_combination.append(selected_combinations)\n",
    "    all_confusion_matrices.append(cm)\n",
    "    d_X_test.append(X_test)\n",
    "    accu.append(acc)\n",
    "    accu_h.append(acc_high)\n",
    "    accu_i.append(acc_inter)\n",
    "    accu_l.append(acc_low)\n",
    "    f1_sc.append(f1_s)\n",
    "    f1_sc_h.append(f1s_high)\n",
    "    f1_sc_i.append(f1s_inter)\n",
    "    f1_sc_l.append(f1s_low)\n",
    "    rec_h.append(rec_high)\n",
    "    rec_i.append(rec_inter)\n",
    "    rec_l.append(rec_low)\n",
    "    spe_h.append(spe_high)\n",
    "    spe_i.append(spe_inter)\n",
    "    spe_l.append(spe_low)\n",
    "    lr_p_h.append(lrp_high)\n",
    "    lr_p_i.append(lrp_inter)\n",
    "    lr_p_l.append(lrp_low)\n",
    "    lr_n_h.append(lrn_high)\n",
    "    lr_n_i.append(lrn_inter)\n",
    "    lr_n_l.append(lrn_low)\n",
    "\n",
    "# Buat DataFrame dari list\n",
    "df_result = pd.DataFrame({'Drug_Combination': drug_combination, \n",
    "                        'X_data': d_X_test, \n",
    "                        'Confusion_Matrix': all_confusion_matrices,\n",
    "                        'Acc' : accu,\n",
    "                        'Acc_High': accu_h,\n",
    "                         'Acc_Inter': accu_i,\n",
    "                         'Acc_Low': accu_l,\n",
    "                        'AUC_High': auc_high,\n",
    "                        'AUC_Inter': auc_inter,\n",
    "                        'AUC_Low': auc_low,\n",
    "                          'Sens_High': rec_h,\n",
    "                          'Sens_Inter': rec_i,\n",
    "                          'Sens_Low': rec_l,\n",
    "                          'Spec_High': spe_h,\n",
    "                          'Spec_Inter': spe_i,\n",
    "                          'Spec_Low': spe_l,\n",
    "                         'F1_S': f1_sc,\n",
    "                         'F1_S_High': f1_sc_h,\n",
    "                         'F1_S_Inter': f1_sc_i,\n",
    "                         'F1_S_Low': f1_sc_l,\n",
    "                         'LR_p_High': lr_p_h,\n",
    "                         'LR_p_Inter': lr_p_i,\n",
    "                         'LR_p_Low': lr_p_l,\n",
    "                         'LR_n_High': lr_n_h,\n",
    "                         'LR_n_Inter': lr_n_i,\n",
    "                         'LR_n_Low': lr_n_l})\n",
    "\n",
    "df_result.to_csv('F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/Result_of_10000_Times_Test_of_RF_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd15eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Define custom colors and edge colors\n",
    "color_high = '#7E8511'  # Custom color for AUC_High\n",
    "color_inter = '#077CDB'  # Custom color for AUC_Inter\n",
    "color_low = '#EC2CAC'  # Custom color for AUC_Low\n",
    "edge_color = 'black'  # Edge color\n",
    "\n",
    "# Create a figure for the histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram for 'AUC_High'\n",
    "plt.subplot(131)\n",
    "plt.hist(auc_high, bins=20, color=color_high, edgecolor=edge_color)\n",
    "plt.title('AUC_High Histogram')\n",
    "plt.xlabel('AUC_High')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for 'AUC_Inter'\n",
    "plt.subplot(132)\n",
    "plt.hist(auc_inter, bins=20, color=color_inter, edgecolor=edge_color)\n",
    "plt.title('AUC_Inter Histogram')\n",
    "plt.xlabel('AUC_Inter')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for 'AUC_Low'\n",
    "plt.subplot(133)\n",
    "plt.hist(auc_low, bins=20, color=color_low, edgecolor=edge_color)\n",
    "plt.title('AUC_Low Histogram')\n",
    "plt.xlabel('AUC_Low')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()  # Ensure the subplots don't overlap\n",
    "\n",
    "# Define the file path where you want to save the histograms\n",
    "output_file_path = 'F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/AUC.png'\n",
    "\n",
    "# Save the figure to the specified file\n",
    "plt.savefig(output_file_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plots (optional)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_result is your DataFrame\n",
    "columns_of_interest = ['Acc', 'Acc_High', 'Acc_Inter', 'Acc_Low', 'AUC_High', 'AUC_Inter', 'AUC_Low', 'F1_S', 'F1_S_High', 'F1_S_Inter', 'F1_S_Low', 'LR_p_High', 'LR_p_Inter', 'LR_p_Low', 'LR_n_High', 'LR_n_Inter', 'LR_n_Low']\n",
    "\n",
    "# Calculate the median and confidence interval\n",
    "medians = df_result[columns_of_interest].median()\n",
    "lower_bound = df_result[columns_of_interest].quantile(0.025)\n",
    "upper_bound = df_result[columns_of_interest].quantile(0.975)\n",
    "\n",
    "# Combine median, lower, and upper into a single string with two decimal places\n",
    "result_strings = medians.round(2).astype(str) + ' (' + lower_bound.round(2).astype(str) + ', ' + upper_bound.round(2).astype(str) + ')'\n",
    "\n",
    "# Create a new DataFrame with the desired format\n",
    "table = pd.DataFrame({'Median (Lower_CI, Upper_CI)': result_strings})\n",
    "\n",
    "# Add a new column for the metric names\n",
    "table['auc_detail'] = columns_of_interest\n",
    "\n",
    "# Reorder the columns\n",
    "table = table[['auc_detail', 'Median (Lower_CI, Upper_CI)']]\n",
    "# Save the table to Excel\n",
    "table.to_excel('F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/Final_result_RF_01.xlsx', index=False)\n",
    "\n",
    "# Display the table\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3751a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats  # Add this import for the stats module\n",
    "\n",
    "# Function to calculate 95% CIs\n",
    "def calculate_ci(data):\n",
    "    mean_val = np.mean(data)\n",
    "    ci_low, ci_high = stats.t.interval(0.95, len(data)-1, loc=mean_val, scale=stats.sem(data))\n",
    "    return ci_low, ci_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to calculate statistics for\n",
    "columns_of_interest = ['Acc', 'Acc_High', 'Acc_Inter', 'Acc_Low', 'AUC_High', 'AUC_Inter', 'AUC_Low', 'Sens_High', 'Sens_Inter','Sens_Low','Spec_High','Spec_Inter','Spec_Low','F1_S', 'F1_S_High', 'F1_S_Inter', 'F1_S_Low', 'LR_p_High', 'LR_p_Inter', 'LR_p_Low', 'LR_n_High', 'LR_n_Inter', 'LR_n_Low']\n",
    "\n",
    "# Calculate median and CIs\n",
    "table = df_result[columns_of_interest].agg(['median', calculate_ci]).T.reset_index()\n",
    "# print(table)\n",
    "table.columns = ['metric', 'median', 'ci']  # Corrected column names\n",
    "print(table)\n",
    "\n",
    "# Save to Excel\n",
    "table.to_excel('D:/backup_CML_1/New_Drugs/Li_Endo/RandomForest_model_03_nestimators_100_maxdepth_None_minsplit_3_minleaf_1/Final_result_RF_01.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f7350",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = df_result[['Acc', 'Acc_High','Acc_Inter','Acc_Low', 'AUC_High', 'AUC_Inter', 'AUC_Low','F1_S','F1_S_High','F1_S_Inter','F1_S_Low','LR_p_High','LR_p_Inter','LR_p_Low', 'LR_n_High', 'LR_n_Inter','LR_n_Low']].agg(['mean', 'median', 'min', 'max']).apply(lambda x: round(x, 2)).reset_index().rename(columns={'index':'auc_detail'})\n",
    "\n",
    "table.to_excel('D:/backup_CML_1/New_Drugs/Li_Endo/RandomForest_model_02_nestimators_300_maxdepth_None_minsplit_5_minleaf_1/Final_result_RF_01.xlsx', index = False)\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293067a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
