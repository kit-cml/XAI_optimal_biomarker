{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cd5c9d",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "import shap\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scaler_np(arr):\n",
    "    mean_val = np.mean(arr)\n",
    "    std_val = np.std(arr)\n",
    "    scaled_arr = (arr - mean_val) / std_val\n",
    "    return scaled_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddafdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A  = pd.read_csv(\"F:/New_Drugs/Li_Endo/Li_Endo_Training.csv\")  # Ganti dengan lokasi dataset A\n",
    "data_A1 = data_A.drop(columns=['Sample_ID', 'Drug_Name', 'risk_level', 'Vm_Peak', 'Ca_Peak', 'Vm_Resting'])\n",
    "print(data_A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb825ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_bfr = data_A1.drop(['risk_code'], axis=1)\n",
    "y_data_label = data_A1['risk_code']\n",
    "print(X_data_bfr)\n",
    "print(y_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f20520",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_data = label_encoder.fit_transform(y_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = standard_scaler_np(X_data_bfr)\n",
    "print(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3328c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of features\n",
    "num_features = 14\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def create_model(units1=6, units2=6, units3=6, optimizer='adam'):\n",
    "    # Create a function to build the model based on hyperparameters\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units1, activation='relu', input_shape=(num_features,)))\n",
    "    model.add(Dense(units2, activation='relu'))\n",
    "    model.add(Dense(units3, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "# Create a KerasClassifier for use in GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'units1': [6,7,8,9,10,11,12,24],\n",
    "    'units2': [6,7,8,9,10,11,12,24],\n",
    "    'units3': [6,7,8,9,10,11,12,24],\n",
    "    'optimizer': ['adam', 'rmsprop']  # Add optimizers to the grid\n",
    "}\n",
    "\n",
    "# Perform the grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=skf, n_jobs=-1)\n",
    "grid_result = grid.fit(X_data, y_data)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_units1 = grid_result.best_params_['units1']\n",
    "best_units2 = grid_result.best_params_['units2']\n",
    "best_units3 = grid_result.best_params_['units3']\n",
    "best_optimizer = grid_result.best_params_['optimizer']\n",
    "unit1 = str(best_units1)\n",
    "unit2 = str(best_units2)\n",
    "unit3 = str(best_units3)\n",
    "\n",
    "# Train the best model using 10-fold cross-validation and 1000 epochs\n",
    "fold = 0\n",
    "for train_index, test_index in skf.split(X_data, y_data):\n",
    "    fold += 1\n",
    "    X_train, X_test = X_data.iloc[train_index], X_data.iloc[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "    model = create_model(units1=best_units1, units2=best_units2, units3=best_units3, optimizer=best_optimizer)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    # Callback to save model at each epoch with epoch number in the filename\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"F:/New_Drugs/Li_Endo/ANN_14_Feature2-node1_{unit1}-node2_{unit2}-node3_{unit3}-oprimizer_{best_optimizer}/Model-{current_time}-fold-{fold}-epoch-{{epoch}}.hdf5\",\n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=False,\n",
    "        mode='min',\n",
    "        save_freq='epoch'\n",
    "    )\n",
    "\n",
    "    # Callback to save training history to a CSV file\n",
    "    csv_logger = CSVLogger(f\"F:/New_Drugs/Li_Endo/ANN_14_Feature2-node1_{unit1}-node2_{unit2}-node3_{unit3}-oprimizer_{best_optimizer}/training_log_fold_{fold}.csv\", separator=',', append=False)\n",
    "\n",
    "    def on_epoch_end(epoch, logs):\n",
    "        # Callback to save training metrics (loss and accuracy) and validation metrics\n",
    "        with open(f\"F:/New_Drugs/Li_Endo/ANN_14_Feature2-node1_{unit1}-node2_{unit2}-node3_{unit3}-oprimizer_{best_optimizer}/training_log_fold_{fold}.csv\", 'a') as file:\n",
    "            file.write(f'{epoch + 1},{logs[\"loss\"]:.4f},{logs[\"accuracy\"]:.4f},{logs[\"val_loss\"]:.4f},{logs[\"val_accuracy\"]:.4f}\\n')\n",
    "\n",
    "    epoch_end_callback = on_epoch_end\n",
    "\n",
    "    callbacks_list = [checkpoint, csv_logger]\n",
    "\n",
    "    # One-hot encode the labels\n",
    "    y_train_encoded = to_categorical(y_train, num_classes=3)\n",
    "    y_test_encoded = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "    model.fit(X_train, y_train_encoded, epochs=1000, batch_size=32, validation_data=(X_test, y_test_encoded), callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9a4bd",
   "metadata": {},
   "source": [
    "# Explainable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0017ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "model_test = load_model(\"F:/New_Drugs/Li_Endo/RBF_20240314-110552/Model11f-001-0.8077.hdf5\", custom_objects={'RBFLayer': RBFLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06bd265",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['risk_level', 'risk_code', 'Sample']\n",
    "encoded_df_column_list = data.columns.difference(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be713174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the first 100 training examples as our background dataset to integrate over\n",
    "import shap\n",
    "explainer = shap.DeepExplainer(model_test, X_test.values)\n",
    "\n",
    "# explain the first 10 predictions\n",
    "# explaining each prediction requires 2 * background dataset size runs\n",
    "shap_values = explainer.shap_values(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assuming 'shap_values' is a list of arrays, where each array corresponds to a class\n",
    "# and 'X_data_test_bfr.columns' gives you the feature names.\n",
    "\n",
    "# Colors and class names for each class\n",
    "class_colors = ['#EC2CAC', '#077CDB', '#7E8511']\n",
    "class_names = ['High-risk', 'Intermediate-risk', 'Low-risk']\n",
    "\n",
    "# Output directory for saving TIFF files\n",
    "output_directory = path2  # Change to your specific directory\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Prepare data for the stacked bar plot\n",
    "feature_names = X_data_test_bfr.columns\n",
    "num_features = len(feature_names)\n",
    "num_classes = len(shap_values)\n",
    "mean_abs_shap_values = np.abs(shap_values).mean(axis=1)  # Mean across samples for each class\n",
    "\n",
    "# Accumulate total SHAP values for each feature\n",
    "total_shap_values = np.sum(mean_abs_shap_values, axis=0)\n",
    "\n",
    "# Sort features by total SHAP values from highest to lowest\n",
    "sorted_indices = np.argsort(total_shap_values)  # The minus sign is for descending order\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "sorted_total_shap_values = total_shap_values[sorted_indices]\n",
    "\n",
    "# Create the sorted stacked bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars_data = np.zeros(num_features)  # Initialize the bottom of the bars\n",
    "\n",
    "for class_index in range(num_classes):\n",
    "    # Sort the mean values according to the sorted feature indices\n",
    "    sorted_mean_vals = mean_abs_shap_values[class_index][sorted_indices]\n",
    "    \n",
    "    # Plot sorted stacked bars\n",
    "    plt.barh(sorted_feature_names, sorted_mean_vals, left=bars_data, color=class_colors[class_index],\n",
    "             edgecolor='black', height=0.5, label=class_names[class_index])\n",
    "    bars_data += sorted_mean_vals  # Update the bottom for the next stack\n",
    "\n",
    "# Add text with the total SHAP value outside of each stacked bar\n",
    "for i, total_val in enumerate(sorted_total_shap_values):\n",
    "    plt.text(bars_data[i], i, f'{total_val:.2f}', ha='left', va='center', color='black', fontsize=8)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Mean (|SHAP Value|)')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Summary of SHAP Values by Class')\n",
    "plt.grid(axis='x')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Save the plot as a TIFF file with DPI 300\n",
    "output_file_path = os.path.join(output_directory, 'SHAP_Summary_Stacked_Plot1.tif')\n",
    "plt.savefig(output_file_path, format='tiff', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Plot saved successfully at: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8b5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(shap_values)):\n",
    "    shap.summary_plot(shap_values[i], X_test.values, feature_names = X_test.columns, show=False)\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.xlabel(\"SHAP Value\")\n",
    "    # Mengubah format penyimpanan menjadi TIFF dan menentukan DPI\n",
    "    plt.savefig(path2 + \"SHAP_Value_HIL_Risk1_{0}.tiff\".format(i), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf377568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Daftar warna yang akan digunakan, satu warna untuk setiap kelas\n",
    "class_colors = ['#077CDB', '#EC2CAC', '#7E8511']\n",
    "\n",
    "# Iterasi untuk setiap kelas\n",
    "for class_index in range(len(shap_values)):\n",
    "    j = str(class_index)\n",
    "    \n",
    "    # Menghitung expected value secara keseluruhan\n",
    "    overall_expected_value = explainer.expected_value[0].numpy()\n",
    "\n",
    "    # Menggabungkan SHAP values dari beberapa baris data (tanpa nilai absolut)\n",
    "    merged_shap_values = np.mean(shap_values[class_index], axis=0)\n",
    "\n",
    "    scaled_shap_values = merged_shap_values * 1000000000\n",
    "\n",
    "    # Memilih warna berdasarkan indeks kelas\n",
    "    class_color = class_colors[class_index % len(class_colors)]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(X_data.columns, scaled_shap_values, color=class_color)\n",
    "    plt.xlabel('SHAP Values')\n",
    "    plt.ylabel('Feature Names')\n",
    "    plt.title(\"SHAP Values for Class_\" + j)\n",
    "    plt.grid(axis='x')\n",
    "    # Menambahkan nilai detail pada setiap batang\n",
    "    # Menambahkan nilai detail pada setiap batang di luar batang, di paling kanan\n",
    "    for bar, shap_value in zip(bars, scaled_shap_values):\n",
    "        plt.text(bar.get_width(), bar.get_y() + bar.get_height() / 2, f'{shap_value:.3f}', ha='left', va='center', color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d882a91b",
   "metadata": {},
   "source": [
    "# 10,000 Times Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_testAs  = pd.read_csv(\"E:/backup_CML_1/New_Drugs/Li_Endo/Li_Endo_Testing.csv\")  # Ganti dengan lokasi dataset A\n",
    "data_testAs1 = data_testAs.drop(columns=['Sample_ID', 'Drug_Name', 'risk_level', 'risk_code'])\n",
    "data_testAs1 = standard_scaler_np(data_testAs1)\n",
    "data_name = data_testAs['Drug_Name']\n",
    "data_test_ID = data_testAs['Sample_ID'].astype(int)\n",
    "data_risk = data_testAs['risk_level']\n",
    "\n",
    "data_test_n = pd.concat([data_test_ID, data_testAs1, data_name, data_risk], axis=1)\n",
    "\n",
    "\n",
    "print(data_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Load model ANN Anda yang sudah dilatih\n",
    "model = model_test\n",
    "\n",
    "# Daftar unik dari jenis 'Drug_Name' dalam dataset\n",
    "unique_drug_names = data_test_n['Drug_Name'].unique()\n",
    "\n",
    "# Inisialisasi list untuk menyimpan semua confusion matrix\n",
    "all_confusion_matrices = []\n",
    "drug_combination = []\n",
    "d_X_test = []\n",
    "accu = []\n",
    "accu_h = []\n",
    "accu_i = []\n",
    "accu_l = []\n",
    "f1_sc = []\n",
    "f1_sc_h = []\n",
    "f1_sc_i = []\n",
    "f1_sc_l = []\n",
    "lr_p_h = []\n",
    "lr_p_i = []\n",
    "lr_p_l = []\n",
    "lr_n_h = []\n",
    "lr_n_i = []\n",
    "lr_n_l = []\n",
    "auc_high = []\n",
    "auc_inter = []\n",
    "auc_low = []\n",
    "\n",
    "# Lakukan iterasi sebanyak 10.000 kali\n",
    "for _ in range(10000):\n",
    "    # Pilih secara acak 16 'Drug_Name' yang berbeda\n",
    "    unique_drug_names = np.random.choice(data_test_n['Drug_Name'].unique(), 16, replace=False)\n",
    "\n",
    "    # Inisialisasi list untuk menyimpan 16 sampel\n",
    "    selected_combinations = []\n",
    "\n",
    "    # Memilih satu sampel dengan 'Drug_Name' yang sesuai untuk setiap 'Drug_Name' yang telah dipilih\n",
    "    for drug_name in unique_drug_names:\n",
    "        selected_sample = data_test_n[data_test_n['Drug_Name'] == drug_name].sample(1)\n",
    "        selected_combinations.append(selected_sample)\n",
    "\n",
    "    # Menggabungkan 16 sampel menjadi satu DataFrame\n",
    "    selected_combinations = pd.concat(selected_combinations)\n",
    "\n",
    "    # Pisahkan fitur dan target sesuai kebutuhan Anda\n",
    "    X_test = selected_combinations.drop(columns=['Sample_ID', 'Drug_Name', 'risk_level', 'Vm_Peak_O', 'Ca_Peak_O', 'Vm_Resting_O'])  # Sesuaikan dengan struktur dataset Anda\n",
    "    y_test = selected_combinations['risk_level']  # Sesuaikan dengan struktur dataset Anda\n",
    "    y_test = label_encoder.fit_transform(y_test)\n",
    "    \n",
    "    # Uji model dengan data uji\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Hitung confusion matrix\n",
    "    cm = confusion_matrix(y_test, np.argmax(y_pred, axis=1))  # Sesuaikan dengan nilai threshold yang sesuai\n",
    "#     print(cm)\n",
    "    \n",
    "    tp_high = cm[0,0]\n",
    "    tn_high = cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2]\n",
    "    fp_high = cm[1,0]+cm[2,0]\n",
    "    fn_high = cm[0,1]+cm[0,2]\n",
    "\n",
    "    tp_inter = cm[1,1]\n",
    "    tn_inter = cm[0,0]+cm[0,2]+cm[2,0]+cm[2,2]\n",
    "    fp_inter = cm[0,1]+cm[2,1]\n",
    "    fn_inter = cm[1,0]+cm[1,2]\n",
    "\n",
    "    tp_low = cm[2,2]\n",
    "    tn_low = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "    fp_low = cm[0,2]+cm[1,2]\n",
    "    fn_low = cm[2,0]+cm[2,1]\n",
    "\n",
    "    acc_high = (tp_high+tn_high)/(tp_high+tn_high+fp_high+fn_high)\n",
    "    pre_high = tp_high/(tp_high+fp_high)\n",
    "    rec_high = tp_high/(tp_high+fn_high)\n",
    "    spe_high = tn_high/(tn_high+fp_high)\n",
    "    lrp_high = rec_high/(1-spe_high)\n",
    "    lrn_high = (1-rec_high)/spe_high\n",
    "    f1s_high = (2*pre_high*rec_high)/(pre_high+rec_high)\n",
    "\n",
    "    acc_inter = (tp_inter+tn_inter)/(tp_inter+tn_inter+fp_inter+fn_inter)\n",
    "    pre_inter = tp_inter/(tp_inter+fp_inter)\n",
    "    rec_inter = tp_inter/(tp_inter+fn_inter)\n",
    "    spe_inter = tn_inter/(tn_inter+fp_inter)\n",
    "    lrp_inter = rec_inter/(1-spe_inter)\n",
    "    lrn_inter = (1-rec_inter)/spe_inter\n",
    "    f1s_inter = (2*pre_inter*rec_inter)/(pre_inter+rec_inter)\n",
    "\n",
    "    acc_low = (tp_low+tn_low)/(tp_low+tn_low+fp_low+fn_low)\n",
    "    pre_low = tp_low/(tp_low+fp_low)\n",
    "    rec_low = tp_low/(tp_low+fn_low)\n",
    "    spe_low = tn_low/(tn_low+fp_low)\n",
    "    lrp_low = rec_low/(1-spe_low)\n",
    "    lrn_low = (1-rec_low)/spe_low\n",
    "    f1s_low = (2*pre_low*rec_low)/(pre_low+rec_low)\n",
    "    \n",
    "    acc = (acc_high+acc_inter+acc_low)/3\n",
    "    f1_s = (f1s_high+f1s_inter+f1s_low)/3\n",
    "    \n",
    "#     fpr_high, tpr_high, _ = roc_curve(y_test == 0, y_pred[:, 0])\n",
    "#     roc_auc_high = auc(fpr_high, tpr_high)\n",
    "    \n",
    "\n",
    "#     fpr_inter, tpr_inter, _ = roc_curve(y_test == 1, y_pred[:, 1])\n",
    "#     roc_auc_inter = auc(fpr_inter, tpr_inter)\n",
    "    \n",
    "\n",
    "#     fpr_low, tpr_low, _ = roc_curve(y_test == 2, y_pred[:, 2])\n",
    "#     roc_auc_low = auc(fpr_low, tpr_low)\n",
    "    \n",
    "    roc_auc_high = roc_auc_score(y_test == 0, y_pred[:, 0])\n",
    "    roc_auc_inter = roc_auc_score(y_test == 1, y_pred[:, 1])\n",
    "    roc_auc_low = roc_auc_score(y_test == 2, y_pred[:, 2])\n",
    "    \n",
    "#     if acc >= 0.70:\n",
    "    auc_high.append(roc_auc_high)\n",
    "    auc_inter.append(roc_auc_inter)\n",
    "    auc_low.append(roc_auc_low)\n",
    "\n",
    "    # Simpan confusion matrix ke dalam list\n",
    "    drug_combination.append(selected_combinations)\n",
    "    all_confusion_matrices.append(cm)\n",
    "    d_X_test.append(X_test)\n",
    "    accu.append(acc)\n",
    "    accu_h.append(acc_high)\n",
    "    accu_i.append(acc_inter)\n",
    "    accu_l.append(acc_low)\n",
    "    f1_sc.append(f1_s)\n",
    "    f1_sc_h.append(f1s_high)\n",
    "    f1_sc_i.append(f1s_inter)\n",
    "    f1_sc_l.append(f1s_low)\n",
    "    lr_p_h.append(lrp_high)\n",
    "    lr_p_i.append(lrp_inter)\n",
    "    lr_p_l.append(lrp_low)\n",
    "    lr_n_h.append(lrn_high)\n",
    "    lr_n_i.append(lrn_inter)\n",
    "    lr_n_l.append(lrn_low)\n",
    "    \n",
    "# Buat DataFrame dari list\n",
    "df_result = pd.DataFrame({'Drug_Combination': drug_combination, \n",
    "                        'X_data': d_X_test, \n",
    "                        'Confusion_Matrix': all_confusion_matrices,\n",
    "                        'Acc' : accu,\n",
    "                        'Acc_High': accu_h,\n",
    "                         'Acc_Inter': accu_i,\n",
    "                         'Acc_Low': accu_l,\n",
    "                        'AUC_High': auc_high,\n",
    "                        'AUC_Inter': auc_inter,\n",
    "                        'AUC_Low': auc_low,\n",
    "                         'F1_S': f1_sc,\n",
    "                         'F1_S_High': f1_sc_h,\n",
    "                         'F1_S_Inter': f1_sc_i,\n",
    "                         'F1_S_Low': f1_sc_l,\n",
    "                         'LR_p_High': lr_p_h,\n",
    "                         'LR_p_Inter': lr_p_i,\n",
    "                         'LR_p_Low': lr_p_l,\n",
    "                         'LR_n_High': lr_n_h,\n",
    "                         'LR_n_Inter': lr_n_i,\n",
    "                         'LR_n_Low': lr_n_l})\n",
    "\n",
    "# df_result_filtered = df_result[df_result['Acc'] >= 0.70]\n",
    "\n",
    "df_result.to_csv('F:/New_Drugs/Li_Endo/RBF_20240314-110552/Result_of_10000_Times_Test_of_Early_Fusion_ANN_22_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Define custom colors and edge colors\n",
    "color_high = '#EC2CAC'  # Custom color for AUC_High\n",
    "color_inter = '#077CDB'  # Custom color for AUC_Inter\n",
    "color_low = '#7E8511'  # Custom color for AUC_Low\n",
    "edge_color = 'black'  # Edge color\n",
    "\n",
    "# Create a figure for the histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram for 'AUC_High'\n",
    "plt.subplot(131)\n",
    "plt.hist(auc_high, bins=20, color=color_high, edgecolor=edge_color)\n",
    "plt.title('AUC_High Histogram')\n",
    "plt.xlabel('AUC_High')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for 'AUC_Inter'\n",
    "plt.subplot(132)\n",
    "plt.hist(auc_inter, bins=20, color=color_inter, edgecolor=edge_color)\n",
    "plt.title('AUC_Inter Histogram')\n",
    "plt.xlabel('AUC_Inter')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for 'AUC_Low'\n",
    "plt.subplot(133)\n",
    "plt.hist(auc_low, bins=20, color=color_low, edgecolor=edge_color)\n",
    "plt.title('AUC_Low Histogram')\n",
    "plt.xlabel('AUC_Low')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()  # Ensure the subplots don't overlap\n",
    "\n",
    "# Define the file path where you want to save the histograms\n",
    "output_file_path = 'F:/New_Drugs/Li_Endo/RBF_20240314-110552/AUC.png'\n",
    "\n",
    "# Save the figure to the specified file\n",
    "plt.savefig(output_file_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plots (optional)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_result is your DataFrame\n",
    "columns_of_interest = ['Acc', 'Acc_High', 'Acc_Inter', 'Acc_Low', 'AUC_High', 'AUC_Inter', 'AUC_Low', 'F1_S', 'F1_S_High', 'F1_S_Inter', 'F1_S_Low', 'LR_p_High', 'LR_p_Inter', 'LR_p_Low', 'LR_n_High', 'LR_n_Inter', 'LR_n_Low']\n",
    "\n",
    "# Calculate the median and confidence interval\n",
    "medians = df_result[columns_of_interest].median()\n",
    "lower_bound = df_result[columns_of_interest].quantile(0.025)\n",
    "upper_bound = df_result[columns_of_interest].quantile(0.975)\n",
    "\n",
    "# Combine median, lower, and upper into a single string with two decimal places\n",
    "result_strings = medians.round(2).astype(str) + ' (' + lower_bound.round(2).astype(str) + ', ' + upper_bound.round(2).astype(str) + ')'\n",
    "\n",
    "# Create a new DataFrame with the desired format\n",
    "table = pd.DataFrame({'Median (Lower_CI, Upper_CI)': result_strings})\n",
    "\n",
    "# Add a new column for the metric names\n",
    "table['auc_detail'] = columns_of_interest\n",
    "\n",
    "# Reorder the columns\n",
    "table = table[['auc_detail', 'Median (Lower_CI, Upper_CI)']]\n",
    "# Save the table to Excel\n",
    "table.to_excel('F:/New_Drugs/Li_Endo/RBF_20240314-110552/Final_result_RF_01.xlsx', index=False)\n",
    "\n",
    "# Display the table\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
