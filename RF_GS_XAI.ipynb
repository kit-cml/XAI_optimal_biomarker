{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668988d8",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "import shap\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_A  = pd.read_csv(\"F:/New_Drugs/Li_Endo/Li_Endo_Training.csv\")  # Ganti dengan lokasi dataset A\n",
    "data_A1 = data_A.drop(columns=['Sample_ID', 'Drug_Name', 'risk_level', 'Vm_Peak', 'Ca_Peak','Ca_Diastole','Max_dVm/dt','Catri','CaD50'])\n",
    "print(data_A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae56539",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_bfr = data_A1.drop(['risk_code'], axis=1)\n",
    "y_data_label = data_A1['risk_code']\n",
    "print(X_data_bfr)\n",
    "print(y_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544120bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100,150,200,250,300],\n",
    "    'max_depth': [None,1,2,3,4],\n",
    "    'min_samples_split': [1,2,3,4,5],\n",
    "    'min_samples_leaf': [1,2,3,4,5]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=skf, n_jobs=-1)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid.fit(X_data_bfr, y_data_label)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid.best_params_\n",
    "\n",
    "# Create a folder name based on the best parameters\n",
    "folder_name = f\"E:/backup_CML_1/New_Drugs/Li_Endo/Result_Revision_2/RF_6_feature_nestimators_{best_params['n_estimators']}_maxdepth_{best_params['max_depth']}_minsplit_{best_params['min_samples_split']}_minleaf_{best_params['min_samples_leaf']}\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Train the best model using 10-fold cross-validation and save models\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X_data_bfr, y_data_label):\n",
    "    fold += 1\n",
    "    X_train, X_test = X_data_bfr.iloc[train_index], X_data_bfr.iloc[test_index]\n",
    "    y_train, y_test = y_data_label[train_index], y_data_label[test_index]\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42, **best_params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Fold {fold} - Akurasi: {accuracy}\")\n",
    "\n",
    "    # Save the model to the folder\n",
    "    model_filename = os.path.join(folder_name, f\"model_fold_{fold}.joblib\")\n",
    "    joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8490c78",
   "metadata": {},
   "source": [
    "# Explainable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "model_test = joblib.load('F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/model_fold_2.joblib')\n",
    "# model_test = load_model(\"F:/7. Result/Chantest_model/save_model_02-node1_12-node2_8-oprimizer_rmsprop/Model-20231017-135318-fold-4-epoch-3.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc94a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model_test)\n",
    "shap_values = explainer.shap_values(X_data_test_bfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb697f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assuming 'shap_values' is a list of arrays, where each array corresponds to a class\n",
    "# and 'X_data_test_bfr.columns' gives you the feature names.\n",
    "\n",
    "# Colors and class names for each class\n",
    "class_colors = ['#EC2CAC', '#077CDB', '#7E8511']\n",
    "class_names = ['High-risk', 'Intermediate-risk', 'Low-risk']\n",
    "\n",
    "# Output directory for saving TIFF files\n",
    "output_directory = path2  # Change to your specific directory\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Prepare data for the stacked bar plot\n",
    "feature_names = df_train.columns\n",
    "num_features = len(feature_names)\n",
    "num_classes = len(shap_values)\n",
    "mean_abs_shap_values = np.abs(shap_values).mean(axis=1)  # Mean across samples for each class\n",
    "\n",
    "# Accumulate total SHAP values for each feature\n",
    "total_shap_values = np.sum(mean_abs_shap_values, axis=0)\n",
    "\n",
    "# Sort features by total SHAP values from highest to lowest\n",
    "sorted_indices = np.argsort(total_shap_values)  # The minus sign is for descending order\n",
    "sorted_feature_names = feature_names[sorted_indices]\n",
    "sorted_total_shap_values = total_shap_values[sorted_indices]\n",
    "\n",
    "# Create the sorted stacked bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars_data = np.zeros(num_features)  # Initialize the bottom of the bars\n",
    "\n",
    "for class_index in range(num_classes):\n",
    "    # Sort the mean values according to the sorted feature indices\n",
    "    sorted_mean_vals = mean_abs_shap_values[class_index][sorted_indices]\n",
    "    \n",
    "    # Plot sorted stacked bars\n",
    "    plt.barh(sorted_feature_names, sorted_mean_vals, left=bars_data, color=class_colors[class_index],\n",
    "             edgecolor='black', height=0.5, label=class_names[class_index])\n",
    "    bars_data += sorted_mean_vals  # Update the bottom for the next stack\n",
    "\n",
    "# Add text with the total SHAP value outside of each stacked bar\n",
    "for i, total_val in enumerate(sorted_total_shap_values):\n",
    "    plt.text(bars_data[i], i, f'{total_val:.2f}', ha='left', va='center', color='black', fontsize=8)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Mean (|SHAP Value|)')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Summary of SHAP Values by Class')\n",
    "plt.grid(axis='x')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Save the plot as a TIFF file with DPI 300\n",
    "output_file_path = os.path.join(output_directory, 'SHAP_Summary_Stacked_Plot1.tif')\n",
    "plt.savefig(output_file_path, format='tiff', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Plot saved successfully at: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7344a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(shap_values)):\n",
    "    shap.summary_plot(shap_values[i], df_train.values, feature_names = df_train.columns, show=False)\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.xlabel(\"SHAP Value\")\n",
    "    # Mengubah format penyimpanan menjadi TIFF dan menentukan DPI\n",
    "    plt.savefig(path2 + \"SHAP_Value_HIL_Risk1_{0}.tiff\".format(i), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "for i in range(len(shap_values)):\n",
    "    # Menghitung expected value secara keseluruhan\n",
    "    overall_expected_value = explainer.expected_value[0]\n",
    "\n",
    "    # Menggabungkan SHAP values dari beberapa baris data\n",
    "    merged_shap_values = shap_values[i].mean(axis=0)\n",
    "\n",
    "    scaled_shap_values = merged_shap_values * 0.1\n",
    "    \n",
    "    plt.figure(figsize=(640, 480))\n",
    "\n",
    "    # Meringkas waterfall plot untuk semua baris data\n",
    "    shap.plots._waterfall.waterfall_legacy(overall_expected_value, scaled_shap_values, \n",
    "                                           feature_names = df_train.columns, max_display=20, show=False)\n",
    "    plt.savefig(path2 + \"Waterfall_HIL_Risk_{0}.tiff\".format(i), dpi=300, format='tiff')\n",
    "    # plt.savefig(\"my_dependence_plot.pdf\") # we can save a PDF of the figure if we want\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07624d79",
   "metadata": {},
   "source": [
    "# 10,000 Times Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_testAs  = pd.read_csv(\"F:/7. Result/running_bu_yunen/duta_testing_ORD.csv\")  # Ganti dengan lokasi dataset A\n",
    "data_testAs1 = data_testAs.drop(columns=['Sample_ID', 'Drug', 'risk_level'])\n",
    "data_name = data_testAs['Drug']\n",
    "data_test_ID = data_testAs['Sample_ID'].astype(int)\n",
    "\n",
    "data_test_n = pd.concat([data_test_ID, data_testAs1, data_name], axis=1)\n",
    "\n",
    "print(data_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10bfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve, auc  # Import roc_curve and auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib  # Import modul joblib\n",
    "\n",
    "# Load the Random Forest model using joblib\n",
    "model = joblib.load('F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/model_fold_2.joblib')  # Ganti dengan path yang sesuai\n",
    "\n",
    "# Daftar unik dari jenis 'Drug_Name' dalam dataset\n",
    "unique_drug_names = data_test_n['Drug'].unique()\n",
    "\n",
    "# Inisialisasi list untuk menyimpan semua confusion matrix\n",
    "all_confusion_matrices = []\n",
    "drug_combination = []\n",
    "d_X_test = []\n",
    "accu = []\n",
    "accu_h = []\n",
    "accu_i = []\n",
    "accu_l = []\n",
    "f1_sc = []\n",
    "f1_sc_h = []\n",
    "f1_sc_i = []\n",
    "f1_sc_l = []\n",
    "lr_p_h = []\n",
    "lr_p_i = []\n",
    "lr_p_l = []\n",
    "lr_n_h = []\n",
    "lr_n_i = []\n",
    "lr_n_l = []\n",
    "auc_high = []\n",
    "auc_inter = []\n",
    "auc_low = []\n",
    "rec_h = []\n",
    "rec_i = []\n",
    "rec_l = []\n",
    "spe_h = []\n",
    "spe_i = []\n",
    "spe_l = []\n",
    "\n",
    "# Lakukan iterasi sebanyak 10.000 kali\n",
    "for _ in range(10000):\n",
    "    # Pilih secara acak 16 'Drug_Name' yang berbeda\n",
    "    unique_drug_names = np.random.choice(data_test_n['Drug'].unique(), 16, replace=False)\n",
    "\n",
    "    # Inisialisasi list untuk menyimpan 16 sampel\n",
    "    selected_combinations = []\n",
    "\n",
    "    # Memilih satu sampel dengan 'Drug_Name' yang sesuai untuk setiap 'Drug_Name' yang telah dipilih\n",
    "    for drug_name in unique_drug_names:\n",
    "        selected_sample = data_test_n[data_test_n['Drug'] == drug_name].sample(1)\n",
    "        selected_combinations.append(selected_sample)\n",
    "\n",
    "    # Menggabungkan 16 sampel menjadi satu DataFrame\n",
    "    selected_combinations = pd.concat(selected_combinations)\n",
    "\n",
    "    # Pisahkan fitur dan target sesuai kebutuhan Anda\n",
    "    X_test = selected_combinations.drop(columns=['Sample_ID', 'risk_code', 'Drug'])  # Sesuaikan dengan struktur dataset Anda\n",
    "    y_test = selected_combinations['risk_code']  # Sesuaikan dengan struktur dataset Anda\n",
    "#     y_test = label_encoder.fit_transform(y_test)\n",
    "    \n",
    "    # Uji model dengan data uji\n",
    "    y_pred = model.predict_proba(X_test)   # Menggunakan model Random Forest yang telah dimuat\n",
    "\n",
    "    # Uji model dengan data uji\n",
    "    y_pred1 = model.predict(X_test)\n",
    "    \n",
    "    # Hitung confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred1)  # Sesuaikan dengan nilai threshold yang sesuai\n",
    "    \n",
    "    tp_high = cm[0,0]\n",
    "    tn_high = cm[1,1]+cm[1,2]+cm[2,1]+cm[2,2]\n",
    "    fp_high = cm[1,0]+cm[2,0]\n",
    "    fn_high = cm[0,1]+cm[0,2]\n",
    "\n",
    "    tp_inter = cm[1,1]\n",
    "    tn_inter = cm[0,0]+cm[0,2]+cm[2,0]+cm[2,2]\n",
    "    fp_inter = cm[0,1]+cm[2,1]\n",
    "    fn_inter = cm[1,0]+cm[1,2]\n",
    "\n",
    "    tp_low = cm[2,2]\n",
    "    tn_low = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "    fp_low = cm[0,2]+cm[1,2]\n",
    "    fn_low = cm[2,0]+cm[2,1]\n",
    "\n",
    "    acc_high = (tp_high+tn_high)/(tp_high+tn_high+fp_high+fn_high)\n",
    "    pre_high = tp_high/(tp_high+fp_high)\n",
    "    rec_high = tp_high/(tp_high+fn_high)\n",
    "    spe_high = tn_high/(tn_high+fp_high)\n",
    "    lrp_high = rec_high/(1-spe_high)\n",
    "    lrn_high = (1-rec_high)/spe_high\n",
    "    f1s_high = (2*pre_high*rec_high)/(pre_high+rec_high)\n",
    "\n",
    "    acc_inter = (tp_inter+tn_inter)/(tp_inter+tn_inter+fp_inter+fn_inter)\n",
    "    pre_inter = tp_inter/(tp_inter+fp_inter)\n",
    "    rec_inter = tp_inter/(tp_inter+fn_inter)\n",
    "    spe_inter = tn_inter/(tn_inter+fp_inter)\n",
    "    lrp_inter = rec_inter/(1-spe_inter)\n",
    "    lrn_inter = (1-rec_inter)/spe_inter\n",
    "    f1s_inter = (2*pre_inter*rec_inter)/(pre_inter+rec_inter)\n",
    "\n",
    "    acc_low = (tp_low+tn_low)/(tp_low+tn_low+fp_low+fn_low)\n",
    "    pre_low = tp_low/(tp_low+fp_low)\n",
    "    rec_low = tp_low/(tp_low+fn_low)\n",
    "    spe_low = tn_low/(tn_low+fp_low)\n",
    "    lrp_low = rec_low/(1-spe_low)\n",
    "    lrn_low = (1-rec_low)/spe_low\n",
    "    f1s_low = (2*pre_low*rec_low)/(pre_low+rec_low)\n",
    "    \n",
    "    acc = (acc_high+acc_inter+acc_low)/3\n",
    "    f1_s = (f1s_high+f1s_inter+f1s_low)/3\n",
    "    \n",
    "# Hitung AUC untuk masing-masing kelas\n",
    "    auc_high_value = roc_auc_score(y_test == 0, y_pred[:, 0])\n",
    "    auc_inter_value = roc_auc_score(y_test == 1, y_pred[:, 1])\n",
    "    auc_low_value = roc_auc_score(y_test == 2, y_pred[:, 2])\n",
    "\n",
    "    auc_high.append(auc_high_value)\n",
    "    auc_inter.append(auc_inter_value)\n",
    "    auc_low.append(auc_low_value)\n",
    "\n",
    "\n",
    "    # Simpan confusion matrix ke dalam list\n",
    "    drug_combination.append(selected_combinations)\n",
    "    all_confusion_matrices.append(cm)\n",
    "    d_X_test.append(X_test)\n",
    "    accu.append(acc)\n",
    "    accu_h.append(acc_high)\n",
    "    accu_i.append(acc_inter)\n",
    "    accu_l.append(acc_low)\n",
    "    f1_sc.append(f1_s)\n",
    "    f1_sc_h.append(f1s_high)\n",
    "    f1_sc_i.append(f1s_inter)\n",
    "    f1_sc_l.append(f1s_low)\n",
    "    rec_h.append(rec_high)\n",
    "    rec_i.append(rec_inter)\n",
    "    rec_l.append(rec_low)\n",
    "    spe_h.append(spe_high)\n",
    "    spe_i.append(spe_inter)\n",
    "    spe_l.append(spe_low)\n",
    "    lr_p_h.append(lrp_high)\n",
    "    lr_p_i.append(lrp_inter)\n",
    "    lr_p_l.append(lrp_low)\n",
    "    lr_n_h.append(lrn_high)\n",
    "    lr_n_i.append(lrn_inter)\n",
    "    lr_n_l.append(lrn_low)\n",
    "\n",
    "# Buat DataFrame dari list\n",
    "df_result = pd.DataFrame({'Drug_Combination': drug_combination, \n",
    "                        'X_data': d_X_test, \n",
    "                        'Confusion_Matrix': all_confusion_matrices,\n",
    "                        'Acc' : accu,\n",
    "                        'Acc_High': accu_h,\n",
    "                         'Acc_Inter': accu_i,\n",
    "                         'Acc_Low': accu_l,\n",
    "                        'AUC_High': auc_high,\n",
    "                        'AUC_Inter': auc_inter,\n",
    "                        'AUC_Low': auc_low,\n",
    "                          'Sens_High': rec_h,\n",
    "                          'Sens_Inter': rec_i,\n",
    "                          'Sens_Low': rec_l,\n",
    "                          'Spec_High': spe_h,\n",
    "                          'Spec_Inter': spe_i,\n",
    "                          'Spec_Low': spe_l,\n",
    "                         'F1_S': f1_sc,\n",
    "                         'F1_S_High': f1_sc_h,\n",
    "                         'F1_S_Inter': f1_sc_i,\n",
    "                         'F1_S_Low': f1_sc_l,\n",
    "                         'LR_p_High': lr_p_h,\n",
    "                         'LR_p_Inter': lr_p_i,\n",
    "                         'LR_p_Low': lr_p_l,\n",
    "                         'LR_n_High': lr_n_h,\n",
    "                         'LR_n_Inter': lr_n_i,\n",
    "                         'LR_n_Low': lr_n_l})\n",
    "\n",
    "df_result.to_csv('F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/Result_of_10000_Times_Test_of_RF_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291cce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Define custom colors and edge colors\n",
    "color_high = '#7E8511'  # Custom color for AUC_High\n",
    "color_inter = '#077CDB'  # Custom color for AUC_Inter\n",
    "color_low = '#EC2CAC'  # Custom color for AUC_Low\n",
    "edge_color = 'black'  # Edge color\n",
    "\n",
    "# Create a figure for the histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram for 'AUC_High'\n",
    "plt.subplot(131)\n",
    "plt.hist(auc_high, bins=20, color=color_high, edgecolor=edge_color)\n",
    "plt.title('AUC_High Histogram')\n",
    "plt.xlabel('AUC_High')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for 'AUC_Inter'\n",
    "plt.subplot(132)\n",
    "plt.hist(auc_inter, bins=20, color=color_inter, edgecolor=edge_color)\n",
    "plt.title('AUC_Inter Histogram')\n",
    "plt.xlabel('AUC_Inter')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Histogram for 'AUC_Low'\n",
    "plt.subplot(133)\n",
    "plt.hist(auc_low, bins=20, color=color_low, edgecolor=edge_color)\n",
    "plt.title('AUC_Low Histogram')\n",
    "plt.xlabel('AUC_Low')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()  # Ensure the subplots don't overlap\n",
    "\n",
    "# Define the file path where you want to save the histograms\n",
    "output_file_path = 'F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/AUC.png'\n",
    "\n",
    "# Save the figure to the specified file\n",
    "plt.savefig(output_file_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plots (optional)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ede39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_result is your DataFrame\n",
    "columns_of_interest = ['Acc', 'Acc_High', 'Acc_Inter', 'Acc_Low', 'AUC_High', 'AUC_Inter', 'AUC_Low', 'F1_S', 'F1_S_High', 'F1_S_Inter', 'F1_S_Low', 'LR_p_High', 'LR_p_Inter', 'LR_p_Low', 'LR_n_High', 'LR_n_Inter', 'LR_n_Low']\n",
    "\n",
    "# Calculate the median and confidence interval\n",
    "medians = df_result[columns_of_interest].median()\n",
    "lower_bound = df_result[columns_of_interest].quantile(0.025)\n",
    "upper_bound = df_result[columns_of_interest].quantile(0.975)\n",
    "\n",
    "# Combine median, lower, and upper into a single string with two decimal places\n",
    "result_strings = medians.round(2).astype(str) + ' (' + lower_bound.round(2).astype(str) + ', ' + upper_bound.round(2).astype(str) + ')'\n",
    "\n",
    "# Create a new DataFrame with the desired format\n",
    "table = pd.DataFrame({'Median (Lower_CI, Upper_CI)': result_strings})\n",
    "\n",
    "# Add a new column for the metric names\n",
    "table['auc_detail'] = columns_of_interest\n",
    "\n",
    "# Reorder the columns\n",
    "table = table[['auc_detail', 'Median (Lower_CI, Upper_CI)']]\n",
    "# Save the table to Excel\n",
    "table.to_excel('F:/7. Result/running_bu_yunen/ord/XGB_14_feature_nestimators_500_maxdepth_6_minchild_4_gamma_0_subsample_0.8_colsample_1.0_lr_0.3/Final_result_RF_01.xlsx', index=False)\n",
    "\n",
    "# Display the table\n",
    "table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
